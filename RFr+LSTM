import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
import os
import tarfile
import zipfile
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import seaborn as sns
from tqdm import tqdm

from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix,classification_report, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import r2_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping

dfD = pd.read_csv('allDataset.csv')
dfD = dfD.dropna()

dfD['datetime']=pd.to_datetime(dfD['datetime'],format="%Y-%m-%d %H:%M:%S")
dfD['month'] = dfD['datetime'].dt.month
dfD['hour'] = dfD['datetime'].dt.hour

X = dfD[['TMA_Lamma', 'RHU_Lamma', 'PRA_Lamma', 'PWV' ,'month', 'hour']] 
y = dfD['RNI_Lamma']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Random Forest regressor
model = RandomForestRegressor(n_estimators=40, random_state=42, max_depth=20, verbose=2)
method='Random Forest'
model.fit(X_train, y_train)

#Instantaneous rain forecast over the entire dataset
p = model.predict(X)
X = X.copy()  
X['pred'] = p  

train_size = int(len(X) * 0.7)  # 70% training
val_size = int(len(X) * 0.15)   # 15% validation

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]
X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]

#Normalization
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_train = (X_train - mean) / std
X_val = (X_val - mean) / std
X_test = (X_test - mean) / std

y_mean = y_train.mean()
y_std = y_train.std()
y_train = (y_train - y_mean) / y_std
y_val = (y_val - y_mean) / y_std
y_test = (y_test - y_mean) / y_std

lookback = 48 # 8h   
delay = 6    #Targets will be 1 hours in the future 
step =  1  # every 10 min  

def split_sequence(samples, target, step, lookback, delay):
    X, y, indices = [], [], []
    print(samples.shape)
    print(target.shape)
    
    #if samples.shape[0] != len(target):
     #return None
    X, y = [], []
    for i in range(0, len(samples) - lookback - delay, step):
        seq_x = samples[i:i + lookback]  
        seq_y = np.max(target[ i + lookback:i + lookback + delay])
        X.append(seq_x)
        y.append(seq_y)
        indices.append(i + lookback)
        
    return np.array(X), np.array(y), np.array(indices)

X_train_seq, y_train_seq, train_indices = split_sequence(X_train.to_numpy(), y_train.to_numpy(), step, lookback, delay)
X_val_seq, y_val_seq, val_indices = split_sequence(X_val.to_numpy(), y_val.to_numpy(), step, lookback, delay)
X_test_seq, y_test_seq, test_indices = split_sequence(X_test.to_numpy(), y_test.to_numpy(), step, lookback, delay)

model = Sequential()
model.add(LSTM(32, return_sequences=True))
model.add(Dropout(0.2)) 
model.add(LSTM(32, return_sequences=False)) 
model.add(Dense(1, activation='linear'))  
model.compile(optimizer='adam', loss='mse')

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
history= model.fit(X_train_seq, y_train_seq,
          epochs=20, 
          batch_size=32, 
          validation_data=(X_val_seq, y_val_seq),
          callbacks=[early_stop], 
          verbose=1)

predictions = model.predict(X_test_seq)
#Denormalisation 
predictions = predictions * y_std + y_mean
y_test_real = y_test_seq * y_std + y_mean 

mae_test = mean_absolute_error(y_test_real, predictions)
r2_test = r2_score(y_test_real, predictions)
mse_test = mean_squared_error(y_test_real, predictions)

print(f'MAE test: {mae_test:.3f}')
print(f'R2 test: {r2_test:.3f}')
print(f'MSE test: {mse_test:.3f}')

plt.figure(figsize=(8, 6))
plt.scatter(y_test_real, predictions, alpha=0.5)
plt.plot([min(y_test_real), max(y_test_real)], [min(y_test_real), max(y_test_real)], 
         color='red', linestyle='--') 

textstr = f'MAE: {mae_test:.3f}\nRÂ²: {r2_test:.3f}\nMSE: {mse_test:.3f}'
plt.text(min(y_test_real), max(predictions), textstr, fontsize=12,
         verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))

plt.xlabel("Real values")
plt.ylabel("predictions")
plt.title("ratio between actual and model predicted values")
plt.grid(True)
plt.show() 
